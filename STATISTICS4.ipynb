{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d69d0fb-561f-49fd-83ec-57ff18868810",
   "metadata": {},
   "source": [
    "## Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63552f99-d86f-4cf5-a88e-b066d6046d37",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fee36-9600-4f4e-8d4c-916a7bca3d7a",
   "metadata": {},
   "source": [
    "1. Probability Mass Function (PMF):\n",
    "\n",
    "a. PMF is used to describe the probability distribution of a discrete random variable.\\\n",
    "b. It provides the probability of the random variable taking on a specific value.\\\n",
    "c. The PMF maps each possible value of the discrete random variable to its probability.\n",
    "\n",
    "Example:\n",
    "Let's say we have a fair six-sided die. The random variable X represents the outcome of rolling the die. The PMF for X would look like this:\n",
    "\n",
    "X = 1: P(X = 1) = 1/6\n",
    "X = 2: P(X = 2) = 1/6\n",
    "X = 3: P(X = 3) = 1/6\n",
    "X = 4: P(X = 4) = 1/6\n",
    "X = 5: P(X = 5) = 1/6\n",
    "X = 6: P(X = 6) = 1/6\n",
    "\n",
    "Here, the PMF tells us the probability of each possible outcome when rolling the die.\n",
    "\n",
    "2. Probability Density Function (PDF):\n",
    "\n",
    "a. PDF is used to describe the probability distribution of a continuous random variable.\\\n",
    "b. It provides the probability density at a given point along the continuous range of possible values.\\\n",
    "c. The area under the PDF curve over a certain interval represents the probability of the random variable falling within that interval.\n",
    "\n",
    "Example:\n",
    "Let's consider a continuous random variable Y representing the height of individuals in a population. The PDF for Y might follow a normal distribution:\n",
    "\n",
    "f(y) = (1 / (σ√(2π))) * exp(-(y - μ)^2 / (2σ^2))\n",
    "\n",
    "In this equation:\n",
    "\n",
    "    μ (mu) represents the mean height of the population.\n",
    "    σ (sigma) represents the standard deviation, which measures the spread of heights.\n",
    "\n",
    "The PDF provides the probability density at any specific height value y. To find the probability of an individual having a height within a certain range (e.g., between a and b), you would integrate the PDF over that interval:\n",
    "\n",
    "P(a ≤ Y ≤ b) = ∫(from a to b) f(y) dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80a472-7372-4c92-a662-1f8d217e5af6",
   "metadata": {},
   "source": [
    "## Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b56bec-a578-4c00-9444-e80877e6628f",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f6e30-9d78-412f-ae84-fdc636101bce",
   "metadata": {},
   "source": [
    "The Cumulative Distribution Function (CDF) is a concept used in probability and statistics to describe the cumulative probability distribution of a random variable. It gives the probability that a random variable takes on a value less than or equal to a specific value.\n",
    "\n",
    "Mathematically, the CDF of a random variable X, denoted as F(x), is defined as:\n",
    "\n",
    "F(x) = P(X ≤ x)\n",
    "\n",
    "In other words, the CDF tells you the probability that the random variable X will be less than or equal to a particular value x.\n",
    "\n",
    "Here's an example to illustrate the CDF:\n",
    "\n",
    "Let's say we have a random variable X representing the number of heads obtained when flipping a fair coin twice. The possible values for X are 0, 1, or 2.\n",
    "\n",
    "The CDF for this random variable would look like this:\n",
    "\n",
    "F(0) = P(X ≤ 0) = P(X = 0) = 1/4 (There's a 1/4 chance of getting no heads)\n",
    "F(1) = P(X ≤ 1) = P(X = 0 or X = 1) = 1/4 + 2/4 = 3/4 (There's a 3/4 chance of getting 1 or fewer heads)\n",
    "F(2) = P(X ≤ 2) = P(X = 0 or X = 1 or X = 2) = 1 (There's a 100% chance of getting 2 or fewer heads)\n",
    "\n",
    "The CDF provides a complete picture of the cumulative probabilities for all possible values of the random variable. It is useful for several reasons:\n",
    "\n",
    "1. Determining Probability Ranges: You can use the CDF to find the probability that a random variable falls within a specific range. For example, you can find P(a ≤ X ≤ b) by subtracting F(a) from F(b).\n",
    "\n",
    "2. Calculating Percentiles: Percentiles are important in various applications, such as standardized testing. You can use the CDF to find the value of x at which F(x) equals a certain percentile (e.g., the 90th percentile).\n",
    "\n",
    "3. Comparison of Distributions: CDFs make it easy to compare different probability distributions. By looking at their CDFs, you can see how the probabilities are distributed across various values.\n",
    "\n",
    "4. Simulation and Modeling: In statistical modeling and simulation, the CDF is often used to generate random numbers that follow a particular probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef631d5-9a4d-4a22-b769-1653651e36e2",
   "metadata": {},
   "source": [
    "## Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82061b-704a-4a8f-8a31-7d8464205af1",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac8320-7e3d-47da-ab26-15816e0000b7",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution or bell curve, is a widely used probability distribution in statistics. It is used as a model in various real-world situations where data tends to be symmetrically distributed around a central value. Some examples of situations where the normal distribution might be used as a model include:\n",
    "\n",
    "1. Height of Individuals: The heights of individuals in a large population tend to follow a normal distribution. This means that most people will be close to the average height, with fewer individuals being either significantly shorter or taller.\n",
    "\n",
    "2. IQ Scores: Intelligence quotient (IQ) scores in a population often approximate a normal distribution. The mean IQ is typically set at 100, and the distribution is symmetrical around this mean.\n",
    "\n",
    "3. Measurement Errors: In scientific experiments and measurements, errors are often normally distributed. This is particularly important when estimating confidence intervals or conducting hypothesis tests.\n",
    "\n",
    "4. Stock Prices: Daily returns of stock prices for many publicly traded companies are often modeled as normally distributed, assuming market efficiency.\n",
    "\n",
    "6. Test Scores: In standardized testing, scores on exams like the SAT or GRE are often assumed to follow a normal distribution. This assumption helps in setting percentile ranks and comparing scores.\n",
    "\n",
    "7. Natural Phenomena: Many natural phenomena, such as the distribution of birth weights, errors in manufacturing processes, and the distribution of reaction times, can often be well-modeled by the normal distribution.\n",
    "\n",
    "Now, let's discuss how the parameters of the normal distribution relate to its shape:\n",
    "\n",
    "1. Mean (μ): The mean is the central value or the center of symmetry for the normal distribution. It determines the location of the peak of the bell curve. If you increase the mean, the entire distribution shifts to the right, and if you decrease it, the distribution shifts to the left.\n",
    "\n",
    "2. Standard Deviation (σ): The standard deviation measures the spread or dispersion of the data. A smaller standard deviation results in a narrower and taller bell curve, indicating that the data points are closely packed around the mean. Conversely, a larger standard deviation results in a wider and flatter curve, indicating greater dispersion of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86222d7b-8d12-429e-a24d-f9be1eb11d99",
   "metadata": {},
   "source": [
    "## Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62548c-a8cc-4291-bdfa-067ae893c02f",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8741b4-d4f9-4f6f-bf8f-8457ef1e7be5",
   "metadata": {},
   "source": [
    "The normal distribution is of significant importance in statistics and data analysis for several reasons:\n",
    "\n",
    "1. Commonly Occurs in Nature: The normal distribution often naturally arises in many real-world phenomena. This makes it a valuable tool for modeling and understanding data in various fields.\n",
    "\n",
    "2. Central Limit Theorem: The central limit theorem states that the sampling distribution of the sample mean of a sufficiently large sample from any population, regardless of the population's distribution, will be approximately normally distributed. This theorem underpins many statistical methods, including hypothesis testing and confidence interval estimation.\n",
    "\n",
    "3. Statistical Inference: Normal distribution simplifies statistical inference. It allows for the development of standardized tests and confidence intervals, making it easier to draw conclusions and make predictions about data.\n",
    "\n",
    "Here are a few real-life examples of situations where the normal distribution is commonly observed:\n",
    "\n",
    "1. Height of Individuals: The heights of individuals in a large population often follow a normal distribution. Most people cluster around the average height, with fewer individuals being significantly shorter or taller.\n",
    "\n",
    "2. IQ Scores: Intelligence quotient (IQ) scores in a population tend to approximate a normal distribution. The mean IQ is usually set at 100, and the distribution is symmetrical around this mean.\n",
    "\n",
    "3. Measurement Errors: Errors in scientific measurements and experiments often follow a normal distribution. This is crucial when estimating confidence intervals and conducting hypothesis tests.\n",
    "\n",
    "4. Stock Prices: Daily returns of stock prices for many publicly traded companies are often modeled as normally distributed, assuming market efficiency.\n",
    "\n",
    "5. Test Scores: In standardized testing, scores on exams like the SAT or GRE are often assumed to follow a normal distribution. This assumption helps in setting percentile ranks and comparing scores.\n",
    "\n",
    "6. Blood Pressure: Blood pressure measurements in a population are often distributed approximately normally, with most individuals having values close to the mean blood pressure.\n",
    "\n",
    "7. Reaction Times: The distribution of reaction times in psychological experiments is often close to normal. This is useful for studying cognitive processes.\n",
    "\n",
    "8. Quality Control: In manufacturing and quality control processes, measurements of product dimensions or defects often follow a normal distribution. This allows for setting tolerances and determining the quality of products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bc5f6-9586-40db-b777-abc85f5703df",
   "metadata": {},
   "source": [
    "## Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f84f4-3255-4816-be35-db351babdbc2",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88396a-f800-4cf5-b480-9d5073472036",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a probability distribution that models a random experiment with two possible outcomes, often referred to as \"success\" and \"failure.\" It is named after the Swiss mathematician Jacob Bernoulli. The distribution is characterized by a single parameter, typically denoted as p, which represents the probability of success.\n",
    "\n",
    "The probability mass function (PMF) of the Bernoulli distribution is as follows:\n",
    "\n",
    "P(X = 1) = p (probability of success)\n",
    "P(X = 0) = 1 - p (probability of failure)\n",
    "\n",
    "Here's an example to illustrate the Bernoulli distribution:\n",
    "\n",
    "Example:\n",
    "Consider a random experiment of flipping a fair coin, where \"Heads\" is considered a success (1) and \"Tails\" is considered a failure (0). The probability of getting Heads (success) is p = 0.5, and the probability of getting Tails (failure) is 1 - p = 0.5.\n",
    "\n",
    "In this case, the Bernoulli distribution can be used to model the outcome of each coin flip. If we let X represent the outcome, X can take values of 1 (Heads) or 0 (Tails), and its probability distribution follows the Bernoulli distribution with p = 0.5.\n",
    "\n",
    "Now, let's discuss the difference between the Bernoulli distribution and the Binomial distribution:\n",
    "\n",
    "1. Number of Trials:\\\n",
    "Bernoulli Distribution: It models a single trial or experiment with two possible outcomes.\\\n",
    "Binomial Distribution: It models the number of successes in a fixed number of independent Bernoulli trials (repeated experiments).\n",
    "\n",
    "2. Parameters:\\\n",
    "Bernoulli Distribution: It has one parameter, p, representing the probability of success in a single trial.\\\n",
    "Binomial Distribution: It has two parameters, n (the number of trials) and p (the probability of success in each trial).\n",
    "\n",
    "3. Number of Possible Outcomes:\\\n",
    "Bernoulli Distribution: It has only two possible outcomes: success (1) and failure (0).\\\n",
    "Binomial Distribution: It can have multiple possible outcomes, representing the number of successes (0, 1, 2,...,n) in a fixed number of trials.\n",
    "\n",
    "4. Probability Mass Function (PMF):\\\n",
    "Bernoulli Distribution: It has a simple PMF with two probabilities (p and 1-p).\\\n",
    "Binomial Distribution: Its PMF calculates the probability of achieving a specific number of successes (k) in n trials using the binomial coefficient.\n",
    "\n",
    "5. Use Cases:\\\n",
    "Bernoulli Distribution: Typically used for modeling single trials with two outcomes, such as success/failure, yes/no, or true/false.\\\n",
    "Binomial Distribution: Used for modeling the number of successes in a series of independent Bernoulli trials, like the number of heads in multiple coin flips or the number of defective items in a sample from a production line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb39b0f-51c5-460e-9efb-d3356848040a",
   "metadata": {},
   "source": [
    "## Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452175b1-8ea4-4b4c-a2b1-4d2d42b35339",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d3a123-f7ea-415b-b139-1a7738d1180e",
   "metadata": {},
   "source": [
    "To find the probability that a randomly selected observation from a normally distributed dataset with a mean (μ) of 50 and a standard deviation (σ) of 10 will be greater than 60, you can use the Z-score formula and standardize the value of 60. Then, you can find the corresponding probability using a standard normal distribution table or calculator.\n",
    "\n",
    "The Z-score formula is as follows:\n",
    "\n",
    "Z = (X - μ) / σ\n",
    "\n",
    "Where:\n",
    "\n",
    "    Z is the Z-score.\n",
    "    X is the value you want to standardize (in this case, 60).\n",
    "    μ is the mean of the dataset (50).\n",
    "    σ is the standard deviation of the dataset (10).\n",
    "\n",
    "Now, plug in the values:\n",
    "\n",
    "Z = (60 - 50) / 10 = 10 / 10 = 1\n",
    "\n",
    "You've standardized the value 60 to a Z-score of 1.\n",
    "\n",
    "Next, you can use a standard normal distribution table or calculator to find the probability associated with a Z-score of 1. The probability corresponds to the area to the right of the Z-score (greater than 1) on the standard normal distribution curve.\n",
    "\n",
    "Using a standard normal distribution table or calculator, you can find that the probability of Z > 1 is approximately 0.1587.\n",
    "\n",
    "So, the probability that a randomly selected observation from the given dataset will be greater than 60 is approximately 0.1587 or 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d1e3e-b1d9-4c76-ac63-637252b40ea9",
   "metadata": {},
   "source": [
    "## Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152670a3-379a-4b3b-a782-4e1bbc5f642b",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfee28-db89-4d19-b789-dccf9a00b071",
   "metadata": {},
   "source": [
    "The uniform distribution is a probability distribution in which all values within a specified range are equally likely to occur. In other words, it's a distribution where every possible outcome has the same probability of occurring. This makes it a simple and straightforward distribution to understand.\n",
    "\n",
    "Mathematically, the probability density function (PDF) of a continuous uniform distribution is defined as follows for a random variable X in the interval [a, b]:\n",
    "\n",
    "f(x) = 1 / (b - a) for a ≤ x ≤ b\n",
    "f(x) = 0 elsewhere\n",
    "\n",
    "Here's an example to illustrate the uniform distribution:\n",
    "\n",
    "Example:\n",
    "Consider a random variable X representing the outcome of rolling a fair six-sided die. In this case, the uniform distribution can be used to model the probability of each possible outcome.\n",
    "\n",
    "    The range of possible values for X is [1, 6] because there are six faces on the die, numbered 1 through 6.\n",
    "    The probability of each outcome is the same because the die is fair, and each face has an equal chance of landing face up.\n",
    "\n",
    "Mathematically, for this uniform distribution:\n",
    "\n",
    "    a = 1 (the minimum value in the range)\n",
    "    b = 6 (the maximum value in the range)\n",
    "    f(x) = 1 / (6 - 1) = 1/5 for x in [1, 6]\n",
    "\n",
    "So, the probability density function (PDF) for X in this case is:\n",
    "\n",
    "f(x) = 1/5 for 1 ≤ x ≤ 6\n",
    "f(x) = 0 elsewhere\n",
    "\n",
    "This means that each possible outcome (1, 2, 3, 4, 5, and 6) has a probability of 1/5 or 20% of occurring when rolling the die. All values within the range [1, 6] have an equal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94db72d-34d6-482f-9988-59558ea0ef24",
   "metadata": {},
   "source": [
    "## Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f1d9e-149d-4d77-8d85-a6ada263e038",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629451f9-67a2-434e-a400-444b2d69afde",
   "metadata": {},
   "source": [
    "The z-score, also known as the standard score or standardized score, is a statistical measure that quantifies the number of standard deviations a data point is from the mean (average) of a dataset. It's a dimensionless value that helps in comparing and understanding the relative position of a data point within a distribution.\n",
    "\n",
    "The formula for calculating the z-score of a data point x in a dataset with a mean (μ) and standard deviation (σ) is as follows:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "Here's why the z-score is important:\n",
    "\n",
    "1. Standardization: The z-score standardizes data, making it easier to compare values from different datasets. By converting data into a common scale (measured in standard deviations), you can compare how different data points relate to the mean of their respective distributions.\n",
    "\n",
    "2. Identification of Outliers: Z-scores can help identify outliers, which are data points that are significantly different from the mean. Data points with z-scores much greater than or less than zero may indicate unusual observations.\n",
    "\n",
    "3. Probability and Normal Distribution: In the context of a normal distribution, z-scores allow you to calculate probabilities associated with specific data values. You can use z-scores to find the probability that a data point falls within a certain range or above/below a threshold.\n",
    "\n",
    "4. Data Transformation: Z-scores are often used to transform data into a standard normal distribution (mean of 0 and standard deviation of 1). This transformation is useful in various statistical analyses and hypothesis testing.\n",
    "\n",
    "5. Quality Control: In quality control and manufacturing processes, z-scores can help in monitoring and controlling the quality of products by flagging data points that deviate significantly from the expected mean.\n",
    "\n",
    "6. Grading and Assessment: In education and testing, z-scores are used to standardize test scores so that they can be compared across different versions of a test or among different groups of test-takers.\n",
    "\n",
    "7. Financial Analysis: In finance, z-scores are used to evaluate the financial stability and creditworthiness of companies. They are a component of models like the Altman Z-score for predicting bankruptcy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60070f9-0628-408a-967c-5c26b81c70db",
   "metadata": {},
   "source": [
    "## Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7d7b4-7e4a-4b83-b888-66456fab2ef9",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e0c6c-ae2f-45b4-8625-c23e62e04896",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the behavior of the sampling distribution of the sample mean (or sum) of a sufficiently large number of independent and identically distributed (i.i.d.) random variables, regardless of the shape of the population distribution. In essence, the CLT states that, as you take more and more samples from a population and calculate their means, the distribution of those sample means will approach a normal distribution, regardless of the original population's distribution.\n",
    "\n",
    "The Central Limit Theorem can be formally stated as follows:\n",
    "\n",
    "Let X₁, X₂, ..., Xₙ be a random sample of n observations from any population with a finite mean (μ) and finite variance (σ²). Then, as n becomes large (typically, n > 30), the sampling distribution of the sample mean (X̄) approaches a normal distribution with mean μ and standard deviation σ/√n.\n",
    "\n",
    "Significance of the Central Limit Theorem:\n",
    "\n",
    "1. Widespread Applicability: The CLT is incredibly important because it applies to a wide range of real-world situations, regardless of the underlying population distribution. This makes it a powerful tool in statistics.\n",
    "\n",
    "2. Foundation for Inference: The CLT forms the basis for many statistical inference methods, such as hypothesis testing, confidence interval estimation, and regression analysis. It allows us to make inferences about population parameters based on sample statistics.\n",
    "\n",
    "3. Sampling from Non-Normal Populations: It enables statisticians to work with sample means from populations that may not necessarily follow a normal distribution. This is particularly useful because many real-world data sets do not have a normal distribution.\n",
    "\n",
    "4. Approximation of Complex Distributions: The CLT allows us to approximate the sampling distribution of the sample mean even when dealing with complex or unknown population distributions. This simplifies statistical analysis.\n",
    "\n",
    "5. Sample Size Determination: The CLT is used to determine sample sizes required for statistical tests and confidence intervals. It helps in ensuring that sample sizes are large enough to rely on the normal approximation.\n",
    "\n",
    "6. Quality Control and Process Monitoring: In manufacturing and quality control, the CLT is used to monitor and control processes by analyzing the distribution of sample means or sums.\n",
    "\n",
    "7. Statistical Education: It is a fundamental concept taught in introductory statistics courses and serves as a building block for more advanced statistical topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce567557-9730-4293-99d8-9814b09241fb",
   "metadata": {},
   "source": [
    "## Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b8e45-e9cf-42f3-b229-201e41e39097",
   "metadata": {},
   "source": [
    "## Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a3bdc-e791-4d1a-a164-37e08e37a3eb",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a powerful statistical concept, but it relies on certain assumptions to hold true. These assumptions are essential for the CLT to apply correctly. Here are the main assumptions of the Central Limit Theorem:\n",
    "\n",
    "1. Independence: The observations in the sample must be independent of each other. This means that the value of one observation should not depend on or influence the value of another observation. Independence is crucial for ensuring that the sample represents a random selection from the population.\n",
    "\n",
    "2. Identical Distribution: The random variables being sampled must be identically distributed. In other words, each observation should come from the same population with the same probability distribution, including the same mean (μ) and variance (σ²).\n",
    "\n",
    "3. Sample Size: While the CLT is often associated with larger sample sizes, it doesn't specify an exact threshold. However, as a rule of thumb, larger sample sizes (typically n > 30) tend to yield better approximations to a normal distribution. Smaller sample sizes may also work if the population distribution is approximately normal or if extreme outliers are rare.\n",
    "\n",
    "4. Finite Variance: The population from which the samples are drawn must have a finite variance (σ²). This assumption ensures that the sample mean converges to a normal distribution.\n",
    "\n",
    "5. Random Sampling: The samples must be selected randomly from the population. This randomness helps ensure that the sample is representative of the population and reduces the risk of bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed970f-8d21-4741-9ccd-3e0e401c12cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
